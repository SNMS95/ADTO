{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SNMS95/AutoDiff_in_TO/blob/main/AD_in_TO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txRsHBaSe2K3"
      },
      "source": [
        "# ðŸ§  Neural Topology Optimization for Compliance Minimization\n",
        "\n",
        "Welcome to this notebook! It runs **out of the box** â€” no installation required â€” and is structured into **4 sections**:\n",
        "\n",
        "---\n",
        "\n",
        "### 1ï¸âƒ£ Physics of the Problem\n",
        "We start with a **direct NumPy translation** of the classic â€œ88 linesâ€ topology optimization code â€” serving as our physics engine for compliance evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### 2ï¸âƒ£ Neural Network Modeling (Keras)\n",
        "We define the neural network architecture using **Keras** in a way that is **agnostic to the machine learning backend** â€” so the same model can be used with either **JAX** or **PyTorch**.\n",
        "\n",
        "---\n",
        "### 3ï¸âƒ£ Writing AD rules\n",
        "We define custom AD rules for JAX and PyTorch using the Implicit function theorem for incorporating a.) the bisection algorithm and b.) Sensitivites from adjoint state method.\n",
        "\n",
        "---\n",
        "\n",
        "### 4ï¸âƒ£ Neural Topology Optimization with AD\n",
        "Finally, we perform topology optimization using **automatic differentiation (AD)** via JAX and PyTorch. We integrate blackbox physics into the computational graph to enable gradient-based optimization.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ› ï¸ Computational Pipeline\n",
        "\n",
        "We follow this simple pipeline:\n",
        "\n",
        "> Neural Network â†’ Enforce Volume Constraint â†’ Density Filter â†’ Linear Solve â†’ Compliance\n",
        "\n",
        "The **FEA solver** is written in **NumPy only** and **do not support AD by default**. To make it differentiable, we write custom vjp rules, separately for each backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d3PP8cOIwvA3"
      },
      "outputs": [],
      "source": [
        "# === User-defined Settings for Neural Topology Optimization ===\n",
        "\n",
        "ML_framework_to_use = \"jax\"  # Choose ML backend: \"torch\" or \"jax\"\n",
        "\n",
        "run_neural_TO = False # If False, runs TO without NN parameterization with OC\n",
        "\n",
        "# Grid resolution (number of finite elements in x and y directions)\n",
        "# âš ï¸ CNN only supports multiples of 8 for Nx and Ny\n",
        "Nx = 3                       # Number of elements along x-axis\n",
        "Ny = 2                       # Number of elements along y-axis\n",
        "\n",
        "# Material properties\n",
        "E0 = 1.0                     # Young's modulus of solid material\n",
        "Emin = 1e-9                  # Young's modulus of void\n",
        "nu = 0.3                     # Poisson's ratio\n",
        "\n",
        "# Filter and penalization\n",
        "rmin = 2.0                   # Radius for density filter\n",
        "penal = 3.0                  # SIMP penalization factor\n",
        "\n",
        "# Optimization control\n",
        "max_iterations = 5         # Number of optimization steps\n",
        "random_seed = 0             # Seed for network initialization\n",
        "volfrac = 0.35\n",
        "# â„¹ï¸ Changing seed can change the starting point of optimization\n",
        "\n",
        "# Optimizer settings\n",
        "# â„¹ï¸ Optimizers from https://keras.io/api/optimizers/\n",
        "optimizer_str = \"adam\"          # Optimizer choice: \"adam\", \"sgd\", \"rmsprop\", or \"adagrad\"\n",
        "optimizer_hyper_params = {\n",
        "    \"learning_rate\": 1e-2,   # Learning rate for the optimizer,\n",
        "    \"global_clipnorm\": 1.0,\n",
        "}\n",
        "\n",
        "# Neural network architecture settings\n",
        "nn_type = \"siren\"  # Choose \"mlp\", \"siren\", or \"cnn\"\n",
        "nn_arch_details = {\n",
        "    # For MLP\n",
        "    'num_hidden_layers': 3,\n",
        "    'hidden_units': 256,\n",
        "    'activation': 'relu',\n",
        "\n",
        "    # For SIREN\n",
        "    'frequency_factor': 30.0,\n",
        "\n",
        "    # For CNN\n",
        "    \"latent_size\": 128\n",
        "                  }\n",
        "\n",
        "# Sanity checks\n",
        "assert ML_framework_to_use in [\"jax\", \"torch\"]\n",
        "assert isinstance(random_seed, int)\n",
        "assert penal >= 1\n",
        "assert rmin >= 1\n",
        "assert isinstance(optimizer_str, str)\n",
        "assert optimizer_str in [\"adam\", \"sgd\", \"rmsprop\", \"adagrad\"]\n",
        "if nn_type.lower() == \"cnn\":\n",
        "  assert Nx % 8 == 0 and Ny % 8 == 0, \"CNN needs multiples of 8 for Nx & Ny\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0li3-B0lRzd"
      },
      "source": [
        "# 1. FEA Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aGS7RhVYpAQG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "from scipy.sparse.linalg import spsolve\n",
        "from scipy.signal import convolve\n",
        "\n",
        "# Set seed for numpy\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "def setup_fea_problem(Nx=64, Ny=32, rmin=2.0, E0=1.0, Emin=1e-6, penal=3.0,\n",
        "                           nu=0.3):\n",
        "    \"\"\"\n",
        "    Precompute all problem-specific matrices and parameters.\n",
        "    Returns a dictionary containing all precomputed data.\n",
        "    \"\"\"\n",
        "    # Element stiffness matrix for 4-node quad element\n",
        "    A11 = np.array([[12, 3, -6, -3], [3, 12, 3, 0],\n",
        "                   [-6, 3, 12, -3], [-3, 0, -3, 12]])\n",
        "    A12 = np.array([[-6, -3, 0, 3], [-3, -6, -3, -6],\n",
        "                   [0, -3, -6, 3], [3, -6, 3, -6]])\n",
        "    B11 = np.array([[-4, 3, -2, 9], [3, -4, -9, 4],\n",
        "                   [-2, -9, -4, -3], [9, 4, -3, -4]])\n",
        "    B12 = np.array([[2, -3, 4, -9], [-3, 2, 9, -2],\n",
        "                   [4, 9, 2, 3], [-9, -2, 3, 2]])\n",
        "\n",
        "    KE = E0/(1-nu**2)/24 * (np.block([[A11, A12], [A12.T, A11]]) +\n",
        "                          nu * np.block([[B11, B12], [B12.T, B11]]))\n",
        "\n",
        "    # DOF connectivity matrix\n",
        "    nodeNrs = np.arange((1 + Nx) * (1 + Ny)).reshape(\n",
        "        (1 + Ny), (1 + Nx), order='F')\n",
        "    cVec = (nodeNrs[:-1, :-1] * 2 + 2).reshape(-1, 1, order='F').ravel()\n",
        "    offsets = np.array([0, 1, 2*Ny + 2, 2*Ny + 3,\n",
        "                       2*Ny, 2*Ny + 1, -2, -1])\n",
        "    cMat = cVec[:, None] + offsets\n",
        "\n",
        "    # Boundary conditions\n",
        "    fixed1 = np.arange(0, 2 * (Ny + 1), 2)  # Fix left edge in x\n",
        "    fixed2 = 2 * nodeNrs[-1, -1] + 1        # Fix bottom-right corner in y\n",
        "    fixed = np.union1d(fixed1, fixed2)\n",
        "\n",
        "    nDof = 2 * (Nx + 1) * (Ny + 1)\n",
        "    free = np.setdiff1d(np.arange(nDof), fixed)\n",
        "\n",
        "    # Load vector\n",
        "    F = np.zeros(nDof)\n",
        "    F[1] = -1.0  # Downward unit load\n",
        "\n",
        "    # # Density filter\n",
        "    range_val = np.arange(-np.ceil(rmin) + 1, np.ceil(rmin))\n",
        "    dx, dy = np.meshgrid(range_val, range_val)\n",
        "    h = np.maximum(0, rmin - np.sqrt(dx**2 + dy**2))\n",
        "    Hs = convolve(np.ones((Ny, Nx)), h, mode='same')\n",
        "\n",
        "    problem_data= {\n",
        "        'Nx': Nx, 'Ny': Ny, 'KE': KE, 'cMat': cMat,\n",
        "        'fixed': fixed, 'free': free, 'F': F,\n",
        "        'E0': E0, 'E_min': Emin, 'penal': penal,\n",
        "        'h': h, 'Hs': Hs\n",
        "    }\n",
        "    return problem_data\n",
        "\n",
        "def assemble_stiffness_matrix(E, problem_data):\n",
        "    \"\"\"Assemble global stiffness matrix\"\"\"\n",
        "    KE, cMat, F = problem_data['KE'], problem_data['cMat'], problem_data['F']\n",
        "    nDof = len(F)\n",
        "\n",
        "    # Build sparse matrix\n",
        "    iK = np.kron(cMat, np.ones((8, 1), dtype=int)).T.ravel(order='F')\n",
        "    jK = np.kron(cMat, np.ones((1, 8), dtype=int)).ravel()\n",
        "    sK = (KE.ravel(order='F')[np.newaxis, :] * E[:, np.newaxis]).ravel()\n",
        "    K = coo_matrix((sK, (iK, jK)), shape=(nDof, nDof)).tocsr()\n",
        "    return K\n",
        "\n",
        "def solve_displacement(K, problem_data):\n",
        "    \"\"\"Solve for displacements\"\"\"\n",
        "    F, free = problem_data['F'], problem_data['free']\n",
        "    u = np.zeros(len(F))\n",
        "    u[free] = spsolve(K[np.ix_(free, free)], F[free])\n",
        "    return u\n",
        "\n",
        "\n",
        "def compute_compliance(xphy, problem_data):\n",
        "    \"\"\"\n",
        "    Compute compliance and its gradient w.r.t. design variables.\n",
        "\n",
        "    This is the main physics function that will be wrapped with custom AD.\n",
        "\n",
        "    Args:\n",
        "        xphy: Physical densities, shape (Nx*Ny,)\n",
        "        problem_data: Dictionary with precomputed problem data\n",
        "\n",
        "    Returns:\n",
        "        compliance: Scalar compliance value\n",
        "        ce: Element-wise compliance gradients, shape (Nx*Ny,)\n",
        "    \"\"\"\n",
        "    # Apply density filter\n",
        "    xphy = xphy.ravel(order='F')\n",
        "\n",
        "    # SIMP material interpolation\n",
        "    E0, E_min, penal = problem_data['E0'], problem_data['E_min'], problem_data['penal']\n",
        "    E = E_min + xphy**penal * (E0 - E_min)\n",
        "\n",
        "    # Assemble and solve\n",
        "    K = assemble_stiffness_matrix(E, problem_data)\n",
        "    u = solve_displacement(K, problem_data)\n",
        "\n",
        "    # Compute element-wise compliance for sensitivity\n",
        "    cMat, KE = problem_data['cMat'], problem_data['KE']\n",
        "    u_elem = u[cMat]\n",
        "    ce_unscaled = np.sum((u_elem @ KE) * u_elem, axis=1)\n",
        "    ce_scaled = E * ce_unscaled\n",
        "    return ce_scaled.sum(), ce_unscaled\n",
        "\n",
        "def bisection_alg(root_fn, x, lb=-10, ub=10, max_iter=100, tol=1e-10):\n",
        "    \"\"\"Standard bisection algorithm to find root of root_fn(eta, fixed_inp) = 0\"\"\"\n",
        "    for _ in range(max_iter):\n",
        "        mid = (lb + ub) / 2\n",
        "        mid_val = root_fn(mid, x)\n",
        "        if mid_val > 0:\n",
        "            ub = mid\n",
        "        else:\n",
        "            lb = mid\n",
        "        if np.abs(mid_val) < tol:\n",
        "            break\n",
        "    return mid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36vDNZuhlby-"
      },
      "source": [
        "# 2. Builidng the NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3UWvUmZqQCb"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = ML_framework_to_use\n",
        "import keras\n",
        "\n",
        "assert keras.backend.backend() == ML_framework_to_use, \"Backend was not set correctly; restart notebook: Runtime/Restart session\"\n",
        "\n",
        "def get_optimizer(opt_str, **hyper_params):\n",
        "  if opt_str == \"adam\":\n",
        "    return keras.optimizers.Adam(**hyper_params)\n",
        "  elif opt_str == \"sgd\":\n",
        "    return keras.optimizers.SGD(**hyper_params)\n",
        "  elif opt_str == \"rmsprop\":\n",
        "    return keras.optimizers.RMSprop(**hyper_params)\n",
        "  elif opt_str == \"adagrad\":\n",
        "    return keras.optimizers.Adagrad(**hyper_params)\n",
        "  else:\n",
        "    raise ValueError(f\"Unsupported optimizer: {opt_str}\")\n",
        "\n",
        "\n",
        "def nn_input(nn_type, Nx=64, Ny=64, latent_size=128):\n",
        "    \"\"\"Generate required neural network inputs\"\"\"\n",
        "    if nn_type in [\"mlp\", \"siren\"]:\n",
        "        # Need coordinates of element centroids\n",
        "        x_centers = np.linspace(-1 + 1/Nx, 1 - 1/Nx, Nx)\n",
        "        y_centers = np.linspace(-1 + 1/Ny, 1 - 1/Ny, Ny)\n",
        "        x_grid, y_grid = np.meshgrid(x_centers, y_centers, indexing='xy')\n",
        "        # Stack coordinates with Fortran-style ordering\n",
        "        input_to_net = np.column_stack([x_grid.ravel(order='F'), y_grid.ravel(order='F')])\n",
        "        return input_to_net\n",
        "    elif nn_type == \"cnn\":\n",
        "        return np.random.normal(size=(latent_size,)).reshape(1, latent_size)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported nn_type: {nn_type}\")\n",
        "\n",
        "def create_network_and_input(nn_type: str = \"mlp\", hyper_params: dict = None,\n",
        "                             random_seed: int = 42, grid_size: tuple = (32, 64)):\n",
        "    \"\"\"Create neural topology optimization model.\"\"\"\n",
        "    keras.backend.clear_session() # We need this to prevent memory overload\n",
        "    keras.utils.set_random_seed(random_seed)\n",
        "\n",
        "    if hyper_params is None:\n",
        "        hyper_params = {}\n",
        "\n",
        "    Ny, Nx = grid_size\n",
        "    latent_size = hyper_params.get(\"latent_size\", 128)\n",
        "\n",
        "    if nn_type == \"mlp\":\n",
        "        n_h_layers = hyper_params.get(\"num_hidden_layers\", 5)\n",
        "        units = hyper_params.get(\"hidden_units\", 20)\n",
        "\n",
        "        inputs = keras.layers.Input(shape=(2,), name='coordinates')\n",
        "        x = inputs\n",
        "        for i in range(n_h_layers):\n",
        "            x = keras.layers.Dense(units)(x)\n",
        "            x = keras.layers.LeakyReLU()(x)\n",
        "            x = keras.layers.BatchNormalization()(x)\n",
        "        outputs = keras.layers.Dense(1)(x)\n",
        "\n",
        "    elif nn_type == \"cnn\":\n",
        "        latent_size = hyper_params.get(\"latent_size\", 128)\n",
        "        activation = hyper_params.get(\"activation\", \"tanh\")\n",
        "\n",
        "        inputs = keras.layers.Input(shape=(latent_size,))\n",
        "        filters = (Ny//8) * (Nx//8) * 32\n",
        "        x = keras.layers.Dense(filters, kernel_initializer=keras.initializers.Orthogonal())(inputs)\n",
        "        x = keras.layers.Reshape([Ny//8, Nx//8, 32])(x)\n",
        "\n",
        "        for resize, nf in zip([1,2,2,2,1], [64,32,16,8,1]):\n",
        "            x = keras.layers.Activation(activation)(x)\n",
        "            x = keras.layers.UpSampling2D((resize, resize), interpolation='bilinear')(x)\n",
        "            x = keras.layers.LayerNormalization()(x)\n",
        "            x = keras.layers.Conv2D(nf, 5, padding=\"same\")(x)\n",
        "        outputs = keras.layers.Reshape([Ny, Nx])(keras.layers.Flatten()(x))\n",
        "\n",
        "    elif nn_type == \"siren\":\n",
        "        omega0 = hyper_params.get(\"frequency_factor\", 30.0)\n",
        "        layers = hyper_params.get(\"num_hidden_layers\", 3)\n",
        "        units = hyper_params.get(\"hidden_units\", 256)\n",
        "\n",
        "        def sine_init(shape, dtype=None, first=False):\n",
        "            limit = 1/shape[0] if first else (6/shape[0])**0.5/omega0\n",
        "            return keras.random.uniform(shape, -limit, limit, seed=random_seed)\n",
        "\n",
        "        inputs = keras.layers.Input(shape=(2,))\n",
        "        x = keras.layers.Dense(units, kernel_initializer=partial(sine_init, first=True))(inputs)\n",
        "        x = keras.ops.sin(x * omega0)\n",
        "        for _ in range(layers-1):\n",
        "            x = keras.ops.sin(keras.layers.Dense(units, kernel_initializer=sine_init)(x) * omega0)\n",
        "        outputs = keras.layers.Dense(1, kernel_initializer=sine_init)(x)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported nn_type: {nn_type}\")\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=f'{nn_type.upper()}')\n",
        "    input_to_net = nn_input(nn_type, Nx=Nx, Ny=Ny, latent_size=latent_size)\n",
        "    # Build model to populate the weights and biases\n",
        "    _ = model(input_to_net)\n",
        "    return model, input_to_net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XErhdLJzPUel"
      },
      "source": [
        "# 3. Integrating blackbox components using custom AD rules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_fp8u72qU4Y"
      },
      "source": [
        "## a. For JAX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXQrTfSnU1L_"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "# Enable float64 for better numerical stability (especially in FEA)\n",
        "jax.config.update(\"jax_enable_x64\", True)\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# ------------------------\n",
        "# Compliance with custom VJP\n",
        "# ------------------------\n",
        "\n",
        "# Declare compliance function as a custom_vjp primitive\n",
        "compute_compliance_ad = jax.custom_vjp(compute_compliance)\n",
        "\n",
        "# Forward pass\n",
        "def compute_compliance_ad_fwd(x, problem_data):\n",
        "    c, ce = compute_compliance(x, problem_data)\n",
        "    residuals = {\n",
        "        \"elem_comp\": ce,         # Element-wise compliance\n",
        "        \"xphy\": x,               # Physical densities\n",
        "        \"penal\": problem_data['penal'],\n",
        "        \"E0\": problem_data[\"E0\"],\n",
        "        \"Emin\": problem_data[\"E_min\"]\n",
        "    }\n",
        "    return (c, ce), residuals  # Outputs and residuals for backward\n",
        "\n",
        "# Backward pass (using analytical gradient)\n",
        "def compute_compliance_ad_bwd(residuals, down_cotangents):\n",
        "    c_dot, ce_dot = down_cotangents  # VJP seed (âˆ‚L/âˆ‚c, âˆ‚L/âˆ‚ce) from upstream\n",
        "    del ce_dot  # ce has no downstream relevance\n",
        "\n",
        "    # Unpack residuals\n",
        "    ce = residuals[\"elem_comp\"]\n",
        "    xphy = residuals[\"xphy\"]\n",
        "    penal = residuals[\"penal\"]\n",
        "    E0 = residuals[\"E0\"]\n",
        "    Emin = residuals[\"Emin\"]\n",
        "\n",
        "    # dC/dx_phys\n",
        "    dc_dxphy = -penal * xphy**(penal - 1) * (E0 - Emin) * ce\n",
        "    vjp_xphys = dc_dxphy * c_dot  # Apply chain rule\n",
        "    return vjp_xphys.reshape(xphy.shape, order='F'), None  # Second arg is âˆ‚L/âˆ‚problem_data = None\n",
        "\n",
        "# Register VJP rules\n",
        "compute_compliance_ad.defvjp(compute_compliance_ad_fwd, compute_compliance_ad_bwd)\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Bisection with custom VJP\n",
        "# ------------------------\n",
        "\n",
        "# Mark bisection as custom_vjp with `root_fn` as nondiff_argnum (it's a Python function)\n",
        "bisection_alg_ad = jax.custom_vjp(bisection_alg, nondiff_argnums=(0,))\n",
        "\n",
        "# Forward pass\n",
        "\n",
        "\n",
        "def bisection_alg_fwd(root_fn, x, lb, ub, max_iter, tol):\n",
        "    eta_star = bisection_alg(root_fn, x, lb, ub, max_iter, tol)\n",
        "    residuals = (x, eta_star)  # Needed for backward\n",
        "    return eta_star, residuals\n",
        "\n",
        "# Backward pass via Implicit Function Theorem\n",
        "\n",
        "\n",
        "def bisection_alg_vjp(root_fn, residuals, down_cotangents):\n",
        "    x, eta_star = residuals\n",
        "    # Compute âˆ‚F/âˆ‚x and âˆ‚F/âˆ‚eta at solution (eta_star s.t. F(x, eta_star) = 0)\n",
        "    df_deta, df_dx = jax.grad(root_fn, (0, 1))(eta_star, x)\n",
        "\n",
        "    # IFT: âˆ‚Î·*/âˆ‚x = - (âˆ‚F/âˆ‚x) / (âˆ‚F/âˆ‚Î·)\n",
        "    lambda_val = down_cotangents / df_deta\n",
        "    vjp_x = -lambda_val * df_dx\n",
        "    return (vjp_x.reshape(x.shape), None, None, None, None)  # Only x is differentiable\n",
        "\n",
        "\n",
        "# Register VJP\n",
        "bisection_alg_ad.defvjp(bisection_alg_fwd, bisection_alg_vjp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgGa1TeC0b-e"
      },
      "source": [
        "## b. For Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEmyOhXD0eX7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class ComplianceAD(torch.autograd.Function):\n",
        "    \"\"\"Custom PyTorch autograd function for compliance computation\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_tensor, problem_data):\n",
        "        \"\"\"Forward pass: compute compliance and save residuals\"\"\"\n",
        "        x_np = x_tensor.detach().cpu().numpy()\n",
        "        c, ce = compute_compliance(x_np, problem_data)\n",
        "\n",
        "        # Save for backward\n",
        "        ctx.save_for_backward(x_tensor, torch.tensor(ce, dtype=x_tensor.dtype))\n",
        "        ctx.problem_data = problem_data\n",
        "\n",
        "        c_tensor = torch.tensor(c, dtype=x_tensor.dtype, device=x_tensor.device)\n",
        "        ce_tensor = torch.tensor(ce, dtype=x_tensor.dtype, device=x_tensor.device)\n",
        "        return c_tensor, ce_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, *grad_outputs):\n",
        "        \"\"\"Backward pass: compute dC/dx\"\"\"\n",
        "        x_tensor, ce_tensor = ctx.saved_tensors\n",
        "        x = x_tensor.detach().cpu().numpy()\n",
        "        ce = ce_tensor.detach().cpu().numpy()\n",
        "\n",
        "        penal = ctx.problem_data['penal']\n",
        "        E0 = ctx.problem_data['E0']\n",
        "        Emin = ctx.problem_data['E_min']\n",
        "\n",
        "        dc_dx = -penal * x**(penal - 1) * (E0 - Emin) * ce\n",
        "        dc_dx_tensor = torch.tensor(dc_dx, dtype=x_tensor.dtype, device=x_tensor.device)\n",
        "\n",
        "        grad_x = grad_outputs[0] * dc_dx_tensor  # grad_output[0] for compliance scalar\n",
        "        return grad_x, None  # Only gradients w.r.t x_tensor\n",
        "\n",
        "\n",
        "class BisectionAD(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, root_fn, lb=-10.0, ub=10.0, max_iter=100, tol=1e-10):\n",
        "        # Call the external root solver (e.g., NumPy-based bisection)\n",
        "            # Convert x to numpy\n",
        "        x_np = x.detach().cpu().numpy()\n",
        "\n",
        "        # Wrap the PyTorch root_fn to accept NumPy and return NumPy\n",
        "        def root_fn_wrapped(eta_np, x_np_local=x_np):\n",
        "            eta_tensor = torch.tensor(eta_np, dtype=x.dtype)\n",
        "            x_tensor = torch.tensor(x_np_local, dtype=x.dtype)\n",
        "            return root_fn(eta_tensor, x_tensor).item()\n",
        "\n",
        "        eta_star = bisection_alg(root_fn_wrapped, x_np, lb, ub, max_iter, tol)\n",
        "        eta_star_tensor = torch.tensor(eta_star, dtype=x.dtype, device=x.device)\n",
        "        ctx.save_for_backward(x, eta_star_tensor)\n",
        "        ctx.root_fn = root_fn\n",
        "        return eta_star_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, eta_star = ctx.saved_tensors\n",
        "        root_fn = ctx.root_fn\n",
        "\n",
        "        # Ensure eta_star is differentiable\n",
        "        eta_star = eta_star.detach().requires_grad_()\n",
        "        x = x.detach().requires_grad_()\n",
        "\n",
        "        # Define f(x, eta*) â‰ˆ 0\n",
        "        def func(eta):\n",
        "            return root_fn(x, eta)\n",
        "\n",
        "        with torch.enable_grad():\n",
        "            f = func(eta_star)\n",
        "            df_deta, = torch.autograd.grad(f, eta_star, retain_graph=True, create_graph=True)\n",
        "            df_dx, = torch.autograd.grad(f, x, retain_graph=True, create_graph=True)\n",
        "\n",
        "        # Apply IFT: dÎ·*/dx = - (âˆ‚F/âˆ‚x) / (âˆ‚F/âˆ‚Î·)\n",
        "        lambda_val = grad_output / df_deta\n",
        "        grad_x = -lambda_val * df_dx\n",
        "\n",
        "        return grad_x, None, None, None, None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSXJOOzkhjrS"
      },
      "source": [
        "# 4. Perform neural topopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27UpOoI2wqda"
      },
      "outputs": [],
      "source": [
        "def apply_density_filter_jax(x, problem_data):\n",
        "    \"\"\"Apply density filter to design variables\"\"\"\n",
        "    Ny, Nx = problem_data['Ny'], problem_data['Nx']\n",
        "    h, Hs = problem_data['h'], problem_data['Hs']\n",
        "    x_2d = x.reshape((Ny, Nx), order='F')\n",
        "    x_filtered = jax.scipy.signal.convolve(x_2d, h, mode='same') / Hs\n",
        "    return x_filtered.ravel(order='F')\n",
        "\n",
        "def root_fn_jax(eta, x, volfrac):\n",
        "    return jax.nn.sigmoid(eta + x).mean() - volfrac\n",
        "\n",
        "def volume_enforce_filter_jax(x_inp, volfrac):\n",
        "    root_fn = partial(root_fn_jax, volfrac=volfrac)\n",
        "    eta_star = jnp.array(bisection_alg_ad(root_fn, x_inp))\n",
        "    return jax.nn.sigmoid(eta_star + x_inp)\n",
        "\n",
        "def train_with_jax_backend():\n",
        "    \"\"\"Training example with JAX backend\"\"\"\n",
        "    print(\"Training with JAX backend...\")\n",
        "\n",
        "    # Setup problem\n",
        "    problem_data = setup_fea_problem(Nx=Nx, Ny=Ny, rmin=rmin, E0=E0, Emin=Emin, penal=penal,\n",
        "                           nu=nu)\n",
        "    # Create nn model\n",
        "    model, model_input = create_network_and_input(nn_type=nn_type, hyper_params=nn_arch_details,\n",
        "                                   random_seed=random_seed, grid_size=(Ny, Nx))\n",
        "\n",
        "    # Create loss function and optimizer\n",
        "    def loss_fn(train_vars, non_train_vars):\n",
        "      # NN call\n",
        "      output, non_train_vars = model.stateless_call(\n",
        "                train_vars, non_train_vars, model_input)\n",
        "      output = output.astype(jnp.float64)\n",
        "      output = volume_enforce_filter_jax(output, volfrac)\n",
        "      output = output.ravel(order='F')\n",
        "      # Apply filter\n",
        "      physical_densities = apply_density_filter_jax(output, problem_data)\n",
        "      # Compute compliance\n",
        "      compliance, ce = compute_compliance_ad(output, problem_data)\n",
        "      return compliance, (non_train_vars, physical_densities)\n",
        "\n",
        "    optimizer = get_optimizer(optimizer_str, **optimizer_hyper_params)\n",
        "\n",
        "    # Training state\n",
        "    trainable_vars = [v.value for v in model.trainable_variables]\n",
        "    non_trainable_vars = model.non_trainable_variables\n",
        "    optimizer.build(model.trainable_variables)\n",
        "    opt_vars = optimizer.variables\n",
        "\n",
        "    # Training loop\n",
        "    losses = []\n",
        "    designs = []\n",
        "    for epoch in range(max_iterations):\n",
        "        (loss, (non_trainable_vars, design)), grads = jax.value_and_grad(\n",
        "            loss_fn, has_aux=True)(trainable_vars, non_trainable_vars)\n",
        "        trainable_vars, opt_vars = optimizer.stateless_apply(\n",
        "            opt_vars, grads, trainable_vars)\n",
        "        losses.append(loss)\n",
        "        designs.append(design)\n",
        "        if epoch % 5 == 0:\n",
        "            print(f\"JAX - Epoch {epoch}, Loss: {loss:.6f}\")\n",
        "\n",
        "    return losses, designs, (trainable_vars, non_trainable_vars, opt_vars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_DMW3zywtjH",
        "outputId": "79486618-fa96-4108-f440-c742210c9d2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with PyTorch backend...\n",
            "PyTorch - Epoch 0, Loss: 5807.802734\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "6603.73583984375"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "def apply_density_filter_torch(x, problem_data):\n",
        "    \"\"\"Apply density filter to design variables (PyTorch version)\"\"\"\n",
        "    Ny, Nx = problem_data['Ny'], problem_data['Nx']\n",
        "    h = problem_data['h']    # Assumed to be a 2D NumPy or torch tensor\n",
        "    Hs = problem_data['Hs']  # Same shape as (Ny, Nx)\n",
        "    # Reshape to 2D with column-major order\n",
        "    x_2d = x.reshape(Ny, Nx)  # PyTorch uses row-major, but reshaping directly works for 1D tensors\n",
        "    # Convert filter kernel to tensor (with proper shape for conv2d)\n",
        "    if not isinstance(h, torch.Tensor):\n",
        "        h = torch.tensor(h, dtype=x.dtype, device=x.device)\n",
        "    kernel = h.unsqueeze(0).unsqueeze(0)  # Shape (1, 1, H, W)\n",
        "    x_input = x_2d.unsqueeze(0).unsqueeze(0)  # Shape (1, 1, Ny, Nx)\n",
        "    # Apply convolution\n",
        "    x_filtered = F.conv2d(x_input, kernel, padding='same')  # Shape (1, 1, Ny, Nx)\n",
        "    # Normalize\n",
        "    if not isinstance(Hs, torch.Tensor):\n",
        "        Hs = torch.tensor(Hs, dtype=x.dtype, device=x.device)\n",
        "    Hs = Hs.unsqueeze(0).unsqueeze(0)  # Shape (1, 1, Ny, Nx)\n",
        "    x_filtered = x_filtered / Hs\n",
        "    x_filtered = x_filtered.squeeze(0).squeeze(0)  # Back to (Ny, Nx)\n",
        "    return x_filtered.reshape(-1)  # Flatten in column-major style\n",
        "\n",
        "\n",
        "def root_fn_torch(eta, x, volfrac):\n",
        "    return torch.sigmoid(eta + x).mean() - volfrac\n",
        "\n",
        "def volume_enforce_filter_torch(x_inp, volfrac):\n",
        "    root_fn = partial(root_fn_torch, volfrac=volfrac)\n",
        "    eta_star = BisectionAD.apply(x_inp, root_fn)\n",
        "    return torch.sigmoid(eta_star + x_inp)\n",
        "\n",
        "def train_with_pytorch_backend():\n",
        "    \"\"\"Training example with PyTorch backend\"\"\"\n",
        "    print(\"Training with PyTorch backend...\")\n",
        "\n",
        "    # Setup problem\n",
        "    problem_data = setup_fea_problem(Nx=Nx, Ny=Ny, rmin=rmin, E0=E0, Emin=Emin, penal=penal,\n",
        "                           nu=nu)\n",
        "    # Create nn model\n",
        "    model, model_input = create_network_and_input(nn_type=nn_type, hyper_params=nn_arch_details,\n",
        "                                   random_seed=random_seed, grid_size=(Ny, Nx))\n",
        "\n",
        "    # Setup optimizer\n",
        "    optimizer = get_optimizer(optimizer_str, **optimizer_hyper_params)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(max_iterations):\n",
        "        with torch.enable_grad():\n",
        "            # Forward pass\n",
        "            densities = model(model_input)\n",
        "            densities = volume_enforce_filter_torch(densities, volfrac)\n",
        "            filtered_densities = apply_density_filter_torch(densities, problem_data)\n",
        "\n",
        "            # Physics simulation\n",
        "            compliance, _ = ComplianceAD.apply(densities.flatten(), problem_data)\n",
        "            # Backward pass\n",
        "            model.zero_grad()\n",
        "            trainable_weights = [v for v in model.trainable_weights]\n",
        "\n",
        "            # Call torch.Tensor.backward() on the loss to compute gradients\n",
        "            # for the weights.\n",
        "            compliance.backward()\n",
        "            gradients = [v.value.grad for v in trainable_weights]\n",
        "\n",
        "            # Update weights\n",
        "            with torch.no_grad():\n",
        "                optimizer.apply(gradients, trainable_weights)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"PyTorch - Epoch {epoch}, Loss: {compliance.item():.6f}\")\n",
        "\n",
        "    return compliance.item()\n",
        "\n",
        "train_with_pytorch_backend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "lYDYun3CzIKM",
        "outputId": "fed7a50a-2637-496a-e53d-d113c49acb9a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-23-980396893.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Greys\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(d[-1].reshape((Ny, Nx), order='F'), cmap=\"Greys\")\n",
        "plt.axis(\"off\")\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# save\n",
        "plt.savefig(\"design_mlp.png\", dpi=300, bbox_inches=\"tight\", pad_inches=0.05)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Loss curve\n",
        "ax1.plot(losses)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Compliance')\n",
        "ax1.set_title('Training Progress')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Final design\n",
        "final_design = designs[-1].reshape(NY, NX, order='F')\n",
        "im = ax2.imshow(final_design, cmap='gray_r', origin='lower')\n",
        "ax2.set_title('Final Design')\n",
        "ax2.set_xlabel('X')\n",
        "ax2.set_ylabel('Y')\n",
        "plt.colorbar(im, ax=ax2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVoV4aZaNO6u",
        "outputId": "4b4deae6-bb03-4e3f-df22-1bc7698d2eb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array(0.34998483, dtype=float64)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d[-1].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2jMtHfGzIAx",
        "outputId": "c67778a1-c953-424d-c5ad-84c5400a98da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with JAX backend...\n",
            "JAX - Epoch 0, Loss: 5785.042610\n"
          ]
        }
      ],
      "source": [
        "l, d, state =train_with_jax_backend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDhdabVqzIMz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw2xR4HGHOWH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMd58PSdlMe/ndELJ9UR3PJ",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
