{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2805d10c",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/SNMS95/AutoDiff_in_TO/blob/main/neuralTO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38426acf",
   "metadata": {},
   "source": [
    "# 🧠 Neural Topology Optimization for Compliance Minimization\n",
    "\n",
    "Welcome to this notebook! It runs **out of the box** — no installation required — and is structured into **4 sections**:\n",
    "\n",
    "\n",
    "### 🛠️ Computational Pipeline\n",
    "\n",
    "We follow this simple pipeline:\n",
    "\n",
    "> Neural Network → Enforce Volume Constraint → Density Filter → Linear Solve → Compliance\n",
    "\n",
    "The **FEA solver** is written in **NumPy only** and **do not support AD by default**. To make it differentiable, we write custom vjp rules, separately for each backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299a7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === User-defined Settings for Neural Topology Optimization ===\n",
    "\n",
    "ML_framework_to_use = \"torch\"  # Choose ML backend: \"torch\" or \"jax\"\n",
    "\n",
    "run_neural_TO = False # If False, runs TO without NN parameterization with OC\n",
    "\n",
    "# Grid resolution (number of finite elements in x and y directions)\n",
    "# ⚠️ CNN only supports multiples of 8 for Nx and Ny\n",
    "Nx = 3                       # Number of elements along x-axis\n",
    "Ny = 2                       # Number of elements along y-axis\n",
    "\n",
    "# Material properties\n",
    "E0 = 1.0                     # Young's modulus of solid material\n",
    "Emin = 1e-9                  # Young's modulus of void\n",
    "nu = 0.3                     # Poisson's ratio\n",
    "\n",
    "# Filter and penalization\n",
    "rmin = 2.0                   # Radius for density filter\n",
    "penal = 3.0                  # SIMP penalization factor\n",
    "\n",
    "# Optimization control\n",
    "max_iterations = 5         # Number of optimization steps\n",
    "random_seed = 0             # Seed for network initialization\n",
    "volfrac = 0.35\n",
    "# ℹ️ Changing seed can change the starting point of optimization\n",
    "\n",
    "# Optimizer settings\n",
    "# ℹ️ Optimizers from https://keras.io/api/optimizers/\n",
    "optimizer_str = \"adam\"          # Optimizer choice: \"adam\", \"sgd\", \"rmsprop\", or \"adagrad\"\n",
    "optimizer_hyper_params = {\n",
    "    \"learning_rate\": 1e-2,   # Learning rate for the optimizer,\n",
    "    \"global_clipnorm\": 1.0,\n",
    "}\n",
    "\n",
    "# Neural network architecture settings\n",
    "nn_type = \"siren\"  # Choose \"mlp\", \"siren\", or \"cnn\"\n",
    "nn_arch_details = {\n",
    "    # For MLP\n",
    "    'num_hidden_layers': 3,\n",
    "    'hidden_units': 256,\n",
    "    'activation': 'relu',\n",
    "\n",
    "    # For SIREN\n",
    "    'frequency_factor': 30.0,\n",
    "\n",
    "    # For CNN\n",
    "    \"latent_size\": 128\n",
    "                  }\n",
    "\n",
    "# Sanity checks\n",
    "assert ML_framework_to_use in [\"jax\", \"torch\"]\n",
    "assert isinstance(random_seed, int)\n",
    "assert penal >= 1\n",
    "assert rmin >= 1\n",
    "assert isinstance(optimizer_str, str)\n",
    "assert optimizer_str in [\"adam\", \"sgd\", \"rmsprop\", \"adagrad\"]\n",
    "if nn_type.lower() == \"cnn\":\n",
    "  assert Nx % 8 == 0 and Ny % 8 == 0, \"CNN needs multiples of 8 for Nx & Ny\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef53f6d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Backend was not set correctly; restart notebook: Runtime/Restart session",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mKERAS_BACKEND\u001b[39m\u001b[33m\"\u001b[39m] = ML_framework_to_use\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m keras.backend.backend() == ML_framework_to_use,\\\n\u001b[32m      6\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mBackend was not set correctly; restart notebook: Runtime/Restart session\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcommon_numpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setup_fea_problem\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_optimizer, create_network_and_input\n",
      "\u001b[31mAssertionError\u001b[39m: Backend was not set correctly; restart notebook: Runtime/Restart session"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = ML_framework_to_use\n",
    "\n",
    "import keras\n",
    "assert keras.backend.backend() == ML_framework_to_use,\\\n",
    "      \"Backend was not set correctly; restart notebook: Runtime/Restart session\"\n",
    "\n",
    "from common_numpy import setup_fea_problem\n",
    "from nn_keras import get_optimizer, create_network_and_input\n",
    "from backend_utils import (compute_compliance_differentiable,\n",
    "                           volume_enforcing_filter,\n",
    "                           apply_density_filter)\n",
    "\n",
    "def train_with_jax_backend():\n",
    "    \"\"\"Training example with JAX backend\"\"\"\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    print(\"Training with JAX backend...\")\n",
    "\n",
    "    # Setup problem\n",
    "    problem_data = setup_fea_problem(Nx=Nx, Ny=Ny, rmin=rmin, E0=E0, Emin=Emin, penal=penal,\n",
    "                           nu=nu)\n",
    "    # Create nn model\n",
    "    model, model_input = create_network_and_input(nn_type=nn_type, hyper_params=nn_arch_details,\n",
    "                                   random_seed=random_seed, grid_size=(Ny, Nx))\n",
    "\n",
    "    # Create loss function and optimizer\n",
    "    def loss_fn(train_vars, non_train_vars):\n",
    "      # NN call\n",
    "      output, non_train_vars = model.stateless_call(\n",
    "                train_vars, non_train_vars, model_input)\n",
    "      output = output.astype(jnp.float64)\n",
    "      output = volume_enforcing_filter(output, volfrac)\n",
    "      output = output.ravel(order='F')\n",
    "      # Apply filter\n",
    "      physical_densities = apply_density_filter(output, problem_data)\n",
    "      # Compute compliance\n",
    "      compliance, ce = compute_compliance_differentiable(output, problem_data)\n",
    "      return compliance, (non_train_vars, physical_densities)\n",
    "\n",
    "    optimizer = get_optimizer(optimizer_str, **optimizer_hyper_params)\n",
    "\n",
    "    # Training state\n",
    "    trainable_vars = [v.value for v in model.trainable_variables]\n",
    "    non_trainable_vars = model.non_trainable_variables\n",
    "    optimizer.build(model.trainable_variables)\n",
    "    opt_vars = optimizer.variables\n",
    "\n",
    "    # Training loop\n",
    "    losses = []\n",
    "    designs = []\n",
    "    for epoch in range(max_iterations):\n",
    "        (loss, (non_trainable_vars, design)), grads = jax.value_and_grad(\n",
    "            loss_fn, has_aux=True)(trainable_vars, non_trainable_vars)\n",
    "        trainable_vars, opt_vars = optimizer.stateless_apply(\n",
    "            opt_vars, grads, trainable_vars)\n",
    "        losses.append(loss)\n",
    "        designs.append(design)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"JAX - Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "    return losses, designs, (trainable_vars, non_trainable_vars, opt_vars)\n",
    "\n",
    "def train_with_pytorch_backend():\n",
    "    \"\"\"Training example with PyTorch backend\"\"\"\n",
    "    import torch\n",
    "    print(\"Training with PyTorch backend...\")\n",
    "\n",
    "    # Setup problem\n",
    "    problem_data = setup_fea_problem(Nx=Nx, Ny=Ny, rmin=rmin, E0=E0, Emin=Emin, penal=penal,\n",
    "                           nu=nu)\n",
    "    # Create nn model\n",
    "    model, model_input = create_network_and_input(nn_type=nn_type, hyper_params=nn_arch_details,\n",
    "                                   random_seed=random_seed, grid_size=(Ny, Nx))\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = get_optimizer(optimizer_str, **optimizer_hyper_params)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(max_iterations):\n",
    "        with torch.enable_grad():\n",
    "            # Forward pass\n",
    "            densities = model(model_input)\n",
    "            densities = volume_enforcing_filter(densities, volfrac)\n",
    "            filtered_densities = apply_density_filter(densities, problem_data)\n",
    "\n",
    "            # Physics simulation\n",
    "            compliance, _ = compute_compliance_differentiable(densities.flatten(), problem_data)\n",
    "            # Backward pass\n",
    "            model.zero_grad()\n",
    "            trainable_weights = [v for v in model.trainable_weights]\n",
    "\n",
    "            # Call torch.Tensor.backward() on the loss to compute gradients\n",
    "            # for the weights.\n",
    "            compliance.backward()\n",
    "            gradients = [v.value.grad for v in trainable_weights]\n",
    "\n",
    "            # Update weights\n",
    "            with torch.no_grad():\n",
    "                optimizer.apply(gradients, trainable_weights)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"PyTorch - Epoch {epoch}, Loss: {compliance.item():.6f}\")\n",
    "\n",
    "    return compliance.item()\n",
    "\n",
    "def train_nn_model():\n",
    "    \"\"\"Train the neural network model.\"\"\"\n",
    "    backend = keras.backend.backend()\n",
    "    if backend == \"jax\":\n",
    "        return train_with_jax_backend()\n",
    "    elif backend == \"pytorch\":\n",
    "        return train_with_pytorch_backend()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown backend: {backend}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abc46e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metatopia_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
